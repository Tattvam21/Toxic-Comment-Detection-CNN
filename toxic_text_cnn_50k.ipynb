{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf000ed",
   "metadata": {},
   "source": [
    "\n",
    "# Toxic Comment Detection — Text CNN (Keras, Single CSV, 50k rows)\n",
    "\n",
    "This notebook trains a **Text CNN** on your single CSV: **`/mnt/data/toxic_comments_50k.csv`**.\n",
    "We'll do an 80/10/10 split (train/val/test), build a CNN with multi-kernel Conv1D, and evaluate with ROC‑AUC & F1.\n",
    "Per‑class thresholds are tuned on the validation set for better F1.\n",
    "\n",
    "> **CSV columns expected:** `id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6f259",
   "metadata": {},
   "source": [
    "## 1) Setup (fast imports; TensorFlow imported later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b5d504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CSV: /Users/tattvam/Documents/ElteBook/Code/Code Unnati 3rd year/Python Script Advanced Course/Project4/toxic_comments_50k.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Setup (fast imports; no TensorFlow yet) ---\n",
    "import os, random, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Try common locations for the CSV (adjust if needed)\n",
    "CANDIDATES = [\n",
    "    Path(\"/mnt/data/toxic_comments_50k.csv\"),   # sandbox path\n",
    "    Path(\"./toxic_comments_50k.csv\"),           # same folder as notebook\n",
    "    Path(\"../toxic_comments_50k.csv\")           # parent folder\n",
    "]\n",
    "\n",
    "CSV_PATH = next((p for p in CANDIDATES if p.exists()), None)\n",
    "assert CSV_PATH is not None, (\n",
    "    \"File not found. Put 'toxic_comments_50k.csv' next to the notebook \"\n",
    "    \"or update CSV_PATH to its correct location.\"\n",
    ")\n",
    "print(\"Using CSV:\", CSV_PATH.resolve())\n",
    "\n",
    "# Artifacts dir\n",
    "ART_DIR = Path(\"./artifacts\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_WORDS  = 30000   # vocab size\n",
    "MAX_LEN    = 200     # tokens per comment\n",
    "EMBED_DIM  = 100     # use 100 if enabling GloVe\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e852f2",
   "metadata": {},
   "source": [
    "## 2) Load CSV & quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35b49740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (50000, 8)\n",
      "Columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>syn_0</td>\n",
       "      <td>This is idiotic. Period.!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>syn_1</td>\n",
       "      <td>I completely disagree with your point.!!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>syn_2</td>\n",
       "      <td>You are so dumb and clueless...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>syn_3</td>\n",
       "      <td>Can you elaborate on this issue?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>syn_4</td>\n",
       "      <td>Thanks for sharing the update. Read the rules.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                    comment_text  toxic  severe_toxic  \\\n",
       "0  syn_0                     This is idiotic. Period.!!!      1             0   \n",
       "1  syn_1        I completely disagree with your point.!!      0             0   \n",
       "2  syn_2                 You are so dumb and clueless...      1             0   \n",
       "3  syn_3                Can you elaborate on this issue?      0             0   \n",
       "4  syn_4  Thanks for sharing the update. Read the rules.      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        0       0       0              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label positives:\n",
      "toxic            14209\n",
      "severe_toxic      1035\n",
      "obscene           3546\n",
      "threat             489\n",
      "insult            5029\n",
      "identity_hate     1532\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "display(df.head())\n",
    "\n",
    "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "for c in label_cols:\n",
    "    assert c in df.columns, f\"Missing label column: {c}\"\n",
    "assert 'comment_text' in df.columns, \"Missing 'comment_text' column\"\n",
    "\n",
    "# Ensure labels are 0/1 ints\n",
    "for c in label_cols:\n",
    "    df[c] = df[c].astype(int)\n",
    "\n",
    "# Basic stats\n",
    "print(\"\\nLabel positives:\")\n",
    "print(df[label_cols].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94386705",
   "metadata": {},
   "source": [
    "## 3) Light text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84692f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is idiotic. Period.!!!</td>\n",
       "      <td>this is idiotic. period.!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I completely disagree with your point.!!</td>\n",
       "      <td>i completely disagree with your point.!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are so dumb and clueless...</td>\n",
       "      <td>you are so dumb and clueless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you elaborate on this issue?</td>\n",
       "      <td>can you elaborate on this issue?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks for sharing the update. Read the rules.</td>\n",
       "      <td>thanks for sharing the update. read the rules.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     comment_text  \\\n",
       "0                     This is idiotic. Period.!!!   \n",
       "1        I completely disagree with your point.!!   \n",
       "2                 You are so dumb and clueless...   \n",
       "3                Can you elaborate on this issue?   \n",
       "4  Thanks for sharing the update. Read the rules.   \n",
       "\n",
       "                                       clean_text  \n",
       "0                     this is idiotic. period.!!!  \n",
       "1        i completely disagree with your point.!!  \n",
       "2                 you are so dumb and clueless...  \n",
       "3                can you elaborate on this issue?  \n",
       "4  thanks for sharing the update. read the rules.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "URL_RE = re.compile(r'http\\S+|www\\.\\S+')\n",
    "USER_RE = re.compile(r'@\\w+')\n",
    "HTML_RE = re.compile(r'<.*?>')\n",
    "SPACE_RE= re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.lower()\n",
    "    s = URL_RE.sub(' URL ', s)\n",
    "    s = USER_RE.sub(' USER ', s)\n",
    "    s = HTML_RE.sub(' ', s)\n",
    "    s = SPACE_RE.sub(' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df['clean_text'] = df['comment_text'].astype(str).apply(clean_text)\n",
    "display(df[['comment_text','clean_text']].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c49f7f",
   "metadata": {},
   "source": [
    "## 4) Tokenize & pad sequences (Keras Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580165d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: (50000, 200) (50000, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='[OOV]')\n",
    "tokenizer.fit_on_texts(df['clean_text'].tolist())\n",
    "\n",
    "def texts_to_padded(texts):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_all = texts_to_padded(df['clean_text'].tolist())\n",
    "y_all = df[label_cols].values.astype('float32')\n",
    "\n",
    "print(\"Tokenized:\", X_all.shape, y_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9a1ec",
   "metadata": {},
   "source": [
    "## 5) Train/Val/Test split (80/10/10, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfd5ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (40000, 200) Val: (5000, 200) Test: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "any_toxic = (y_all.sum(axis=1) > 0).astype(int)\n",
    "\n",
    "# First split train vs temp\n",
    "X_train, X_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(\n",
    "    X_all, y_all, np.arange(len(y_all)), test_size=0.2, random_state=SEED, stratify=any_toxic\n",
    ")\n",
    "\n",
    "# Split temp into val/test\n",
    "any_toxic_temp = (y_temp.sum(axis=1) > 0).astype(int)\n",
    "X_val, X_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
    "    X_temp, y_temp, idx_temp, test_size=0.5, random_state=SEED, stratify=any_toxic_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f972fd9",
   "metadata": {},
   "source": [
    "## 6) (Optional) Load GloVe embeddings (skip if not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ee2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe not found; will train embeddings from scratch.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "GLOVE_PATH = Path(\"/mnt/data/glove.6B.100d.txt\")  # put file here to enable\n",
    "emb_matrix = None\n",
    "\n",
    "if GLOVE_PATH.exists():\n",
    "    print(\"Loading GloVe from\", GLOVE_PATH)\n",
    "    embeddings_index = {}\n",
    "    with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    word_index = tokenizer.word_index\n",
    "    num_tokens = min(MAX_WORDS, len(word_index) + 1)\n",
    "    emb_matrix = np.random.normal(0, 0.6, (num_tokens, EMBED_DIM)).astype('float32')\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_WORDS: \n",
    "            continue\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            emb_matrix[i] = vec\n",
    "    print(\"Built embedding matrix:\", emb_matrix.shape)\n",
    "else:\n",
    "    print(\"GloVe not found; will train embeddings from scratch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e86f5",
   "metadata": {},
   "source": [
    "## 7) Build Text CNN model (lazy-import TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1585b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ tokens (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │ tokens[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">38,528</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,328</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">196</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,310</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ tokens (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │  \u001b[38;5;34m3,000,000\u001b[0m │ tokens[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m38,528\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m51,328\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m196\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m64,128\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │      \u001b[38;5;34m2,310\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,156,294</span> (12.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,156,294\u001b[0m (12.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,156,294</span> (12.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,156,294\u001b[0m (12.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Speed up TF import/startup for CPU-only\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"     # skip GPU probing if no CUDA\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"2\"  # optional: tune threads\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"]  = \"4\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "                                     Concatenate, Dense, Dropout)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def build_text_cnn(max_words, max_len, embed_dim, emb_matrix=None, num_labels=6, filters=128, drop=0.5):\n",
    "    inp = Input(shape=(max_len,), name=\"tokens\")\n",
    "    if emb_matrix is not None:\n",
    "        emb = Embedding(input_dim=emb_matrix.shape[0],\n",
    "                        output_dim=emb_matrix.shape[1],\n",
    "                        weights=[emb_matrix],\n",
    "                        input_length=max_len,\n",
    "                        trainable=False, name=\"embedding\")(inp)\n",
    "    else:\n",
    "        emb = Embedding(input_dim=max_words,\n",
    "                        output_dim=embed_dim,\n",
    "                        input_length=max_len,\n",
    "                        name=\"embedding\")(inp)\n",
    "\n",
    "    convs = []\n",
    "    for k in [3,4,5]:\n",
    "        c = Conv1D(filters, kernel_size=k, activation='relu', padding='valid')(emb)\n",
    "        p = GlobalMaxPooling1D()(c)\n",
    "        convs.append(p)\n",
    "    x = Concatenate()(convs)\n",
    "    x = Dropout(drop)(x)\n",
    "    out = Dense(num_labels, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(2e-3),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.AUC(name='auc', multi_label=True)])\n",
    "    return model\n",
    "\n",
    "model = build_text_cnn(MAX_WORDS, MAX_LEN, EMBED_DIM, emb_matrix, num_labels=len(label_cols))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f8d56",
   "metadata": {},
   "source": [
    "## 8) Train (EarlyStopping on val AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a1d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 45ms/step - auc: 0.9853 - loss: 0.0469 - val_auc: 0.9965 - val_loss: 0.0226 - learning_rate: 0.0020\n",
      "Epoch 2/6\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - auc: 0.9968 - loss: 0.0223\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 47ms/step - auc: 0.9968 - loss: 0.0221 - val_auc: 0.9965 - val_loss: 0.0222 - learning_rate: 0.0020\n",
      "Epoch 3/6\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - auc: 0.9965 - loss: 0.0215\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 48ms/step - auc: 0.9967 - loss: 0.0213 - val_auc: 0.9966 - val_loss: 0.0223 - learning_rate: 0.0010\n",
      "Epoch 4/6\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 48ms/step - auc: 0.9970 - loss: 0.0208 - val_auc: 0.9966 - val_loss: 0.0221 - learning_rate: 5.0000e-04\n",
      "Epoch 5/6\n",
      "\u001b[1m624/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - auc: 0.9969 - loss: 0.0208\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 48ms/step - auc: 0.9970 - loss: 0.0207 - val_auc: 0.9966 - val_loss: 0.0221 - learning_rate: 5.0000e-04\n",
      "Epoch 6/6\n",
      "\u001b[1m624/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - auc: 0.9970 - loss: 0.0207\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 48ms/step - auc: 0.9971 - loss: 0.0205 - val_auc: 0.9966 - val_loss: 0.0221 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', mode='max', patience=2, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_auc', mode='max', factor=0.5, patience=1, min_lr=1e-5, verbose=1)\n",
    "]\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048edec",
   "metadata": {},
   "source": [
    "## 9) Evaluate on validation & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d73eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC-AUC per class: {'toxic': 0.9797147327074762, 'severe_toxic': 1.0, 'obscene': 1.0, 'threat': 1.0, 'insult': 1.0, 'identity_hate': 1.0}\n",
      "Validation Macro ROC-AUC: 0.9966\n",
      "\n",
      "Validation classification report (thr=0.5):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.79      1.00      0.88      1381\n",
      " severe_toxic       1.00      1.00      1.00        98\n",
      "      obscene       1.00      1.00      1.00       371\n",
      "       threat       1.00      0.98      0.99        48\n",
      "       insult       1.00      1.00      1.00       519\n",
      "identity_hate       1.00      1.00      1.00       157\n",
      "\n",
      "    micro avg       0.87      1.00      0.93      2574\n",
      "    macro avg       0.96      1.00      0.98      2574\n",
      " weighted avg       0.89      1.00      0.94      2574\n",
      "  samples avg       0.31      0.35      0.33      2574\n",
      "\n",
      "Test ROC-AUC per class: {'toxic': 0.9832490529383197, 'severe_toxic': 1.0, 'obscene': 1.0, 'threat': 1.0, 'insult': 1.0, 'identity_hate': 1.0}\n",
      "Test Macro ROC-AUC: 0.9972\n",
      "\n",
      "Test classification report (thr=0.5):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.82      1.00      0.90      1450\n",
      " severe_toxic       1.00      1.00      1.00        92\n",
      "      obscene       1.00      1.00      1.00       341\n",
      "       threat       1.00      1.00      1.00        47\n",
      "       insult       1.00      1.00      1.00       498\n",
      "identity_hate       1.00      1.00      1.00       152\n",
      "\n",
      "    micro avg       0.89      1.00      0.94      2580\n",
      "    macro avg       0.97      1.00      0.98      2580\n",
      " weighted avg       0.90      1.00      0.95      2580\n",
      "  samples avg       0.32      0.35      0.33      2580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_probs(y_true, y_prob, label_cols):\n",
    "    # ROC-AUC per class\n",
    "    aucs = {}\n",
    "    for i, lab in enumerate(label_cols):\n",
    "        try:\n",
    "            aucs[lab] = roc_auc_score(y_true[:, i], y_prob[:, i])\n",
    "        except ValueError:\n",
    "            aucs[lab] = np.nan\n",
    "    macro_auc = np.nanmean(list(aucs.values()))\n",
    "    # F1 at 0.5 threshold\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_cols, zero_division=0)\n",
    "    return aucs, macro_auc, report\n",
    "\n",
    "# Validation\n",
    "y_val_prob = model.predict(X_val, batch_size=256, verbose=0)\n",
    "val_aucs, val_macro_auc, val_report = evaluate_probs(y_val, y_val_prob, label_cols)\n",
    "print(\"Validation ROC-AUC per class:\", val_aucs)\n",
    "print(f\"Validation Macro ROC-AUC: {val_macro_auc:.4f}\")\n",
    "print(\"\\nValidation classification report (thr=0.5):\\n\", val_report)\n",
    "\n",
    "# Test\n",
    "y_test_prob = model.predict(X_test, batch_size=256, verbose=0)\n",
    "test_aucs, test_macro_auc, test_report = evaluate_probs(y_test, y_test_prob, label_cols)\n",
    "print(\"Test ROC-AUC per class:\", test_aucs)\n",
    "print(f\"Test Macro ROC-AUC: {test_macro_auc:.4f}\")\n",
    "print(\"\\nTest classification report (thr=0.5):\\n\", test_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd62d7d",
   "metadata": {},
   "source": [
    "## 10) Tune per-class thresholds (maximize F1 on validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ef10b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-class optimal thresholds:\n",
      "{'toxic': np.float64(0.525), 'severe_toxic': np.float64(0.05), 'obscene': np.float64(0.05), 'threat': np.float64(0.05), 'insult': np.float64(0.05), 'identity_hate': np.float64(0.05)}\n",
      "\n",
      "Test report with tuned thresholds:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.82      0.99      0.90      1450\n",
      " severe_toxic       1.00      1.00      1.00        92\n",
      "      obscene       1.00      1.00      1.00       341\n",
      "       threat       1.00      1.00      1.00        47\n",
      "       insult       1.00      1.00      1.00       498\n",
      "identity_hate       1.00      1.00      1.00       152\n",
      "\n",
      "    micro avg       0.89      1.00      0.94      2580\n",
      "    macro avg       0.97      1.00      0.98      2580\n",
      " weighted avg       0.90      1.00      0.94      2580\n",
      "  samples avg       0.32      0.35      0.33      2580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimal_thresholds(probs, targets, steps=200):\n",
    "    thrs = []\n",
    "    for i in range(probs.shape[1]):\n",
    "        best_f1, best_t = 0.0, 0.5\n",
    "        for t in np.linspace(0.05, 0.95, steps):\n",
    "            f1 = f1_score(targets[:, i], (probs[:, i] >= t).astype(int), zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "        thrs.append(best_t)\n",
    "    return np.array(thrs)\n",
    "\n",
    "thr_vec = optimal_thresholds(y_val_prob, y_val)\n",
    "print(\"Per-class optimal thresholds:\")\n",
    "print(dict(zip(label_cols, np.round(thr_vec, 3))))\n",
    "\n",
    "# Re-evaluate on test with tuned thresholds\n",
    "y_test_pred_tuned = (y_test_prob >= thr_vec).astype(int)\n",
    "print(\"\\nTest report with tuned thresholds:\")\n",
    "print(classification_report(y_test, y_test_pred_tuned, target_names=label_cols, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a625ba3",
   "metadata": {},
   "source": [
    "## 11) Save artifacts (model, tokenizer, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17ba050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: /Users/tattvam/Documents/ElteBook/Code/Code Unnati 3rd year/Python Script Advanced Course/Project4/artifacts/text_cnn_toxic.keras\n",
      "Saved tokenizer to: /Users/tattvam/Documents/ElteBook/Code/Code Unnati 3rd year/Python Script Advanced Course/Project4/artifacts/tokenizer.json\n",
      "Saved thresholds to: /Users/tattvam/Documents/ElteBook/Code/Code Unnati 3rd year/Python Script Advanced Course/Project4/artifacts/thresholds.npy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use Keras v3 format to avoid HDF5 dependency\n",
    "model_path = ART_DIR / \"text_cnn_toxic.keras\"\n",
    "tok_path   = ART_DIR / \"tokenizer.json\"\n",
    "thr_path   = ART_DIR / \"thresholds.npy\"\n",
    "\n",
    "with open(tok_path, \"w\") as f:\n",
    "    f.write(tokenizer.to_json())\n",
    "np.save(thr_path, thr_vec)\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "print(\"Saved model to:\", model_path.resolve())\n",
    "print(\"Saved tokenizer to:\", tok_path.resolve())\n",
    "print(\"Saved thresholds to:\", thr_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918c67e",
   "metadata": {},
   "source": [
    "## 12) Inference helper (single text or list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "542c88e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions (probabilities):\n",
      " [[0.009 0.    0.046 0.    0.    0.001]\n",
      " [0.982 0.    0.    0.176 1.    0.   ]\n",
      " [0.796 0.    0.    1.    0.    0.   ]]\n",
      "Sample predictions (labels):\n",
      " [[0 0 0 0 0 0]\n",
      " [1 0 0 1 1 0]\n",
      " [1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_texts(texts, model, tokenizer, max_len, thresholds=None):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    # reuse cleaner\n",
    "    cleaned = [clean_text(t) for t in texts]\n",
    "    seqs = tokenizer.texts_to_sequences(cleaned)\n",
    "    X = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "    probs = model.predict(X, verbose=0)\n",
    "    if thresholds is None:\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "    else:\n",
    "        preds = (probs >= thresholds).astype(int)\n",
    "    return probs, preds\n",
    "\n",
    "sample_texts = [\n",
    "    \"Thanks for the detailed explanation, much appreciated!\",\n",
    "    \"You're clueless and incompetent, just stop talking.\",\n",
    "    \"Keep talking and you'll regret it.\"\n",
    "]\n",
    "probs, preds = predict_texts(sample_texts, model, tokenizer, MAX_LEN, thresholds=thr_vec)\n",
    "print(\"Sample predictions (probabilities):\\n\", probs.round(3))\n",
    "print(\"Sample predictions (labels):\\n\", preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888fe45",
   "metadata": {},
   "source": [
    "## 13) (Optional) Baseline: TF‑IDF + Logistic Regression (fast sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89825e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': {'auc': 0.9787, 'f1@0.5': 0.8786},\n",
       " 'severe_toxic': {'auc': 1.0, 'f1@0.5': 0.9462},\n",
       " 'obscene': {'auc': 1.0, 'f1@0.5': 0.9959},\n",
       " 'threat': {'auc': 1.0, 'f1@0.5': 0.9333},\n",
       " 'insult': {'auc': 1.0, 'f1@0.5': 0.9952},\n",
       " 'identity_hate': {'auc': 1.0, 'f1@0.5': 0.9739}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Small baseline on train/val split only (to keep it quick)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=100000, min_df=2)\n",
    "X_tr_tfidf = tfidf.fit_transform(df.loc[idx_train, 'clean_text'].tolist())\n",
    "X_va_tfidf = tfidf.transform(df.loc[idx_val, 'clean_text'].tolist())\n",
    "\n",
    "lr_reports = {}\n",
    "for i, lab in enumerate(label_cols):\n",
    "    lr = LogisticRegression(max_iter=200, n_jobs=None)\n",
    "    lr.fit(X_tr_tfidf, y_train[:, i])\n",
    "    val_prob = lr.predict_proba(X_va_tfidf)[:, 1]\n",
    "    auc = roc_auc_score(y_val[:, i], val_prob)\n",
    "    pred = (val_prob >= 0.5).astype(int)\n",
    "    f1 = f1_score(y_val[:, i], pred, zero_division=0)\n",
    "    lr_reports[lab] = {\"auc\": round(auc, 4), \"f1@0.5\": round(f1, 4)}\n",
    "lr_reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412856b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
