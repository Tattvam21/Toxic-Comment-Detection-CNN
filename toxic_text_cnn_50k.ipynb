{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf000ed",
   "metadata": {},
   "source": [
    "\n",
    "# Toxic Comment Detection — Text CNN (Keras, Single CSV, 50k rows)\n",
    "\n",
    "This notebook trains a **Text CNN** on your single CSV: **`/mnt/data/toxic_comments_50k.csv`**.\n",
    "We'll do an 80/10/10 split (train/val/test), build a CNN with multi-kernel Conv1D, and evaluate with ROC‑AUC & F1.\n",
    "Per‑class thresholds are tuned on the validation set for better F1.\n",
    "\n",
    "> **CSV columns expected:** `id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6f259",
   "metadata": {},
   "source": [
    "## 1) Setup (fast imports; TensorFlow imported later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep imports light to avoid slow startup\n",
    "import os, random, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Paths & config\n",
    "CSV_PATH   = Path(\"/mnt/data/toxic_comments_50k.csv\")\n",
    "ART_DIR    = Path(\"./artifacts\"); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAX_WORDS  = 30000   # vocab size\n",
    "MAX_LEN    = 200     # tokens per comment\n",
    "EMBED_DIM  = 100     # use 100 if enabling GloVe\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 6\n",
    "\n",
    "assert CSV_PATH.exists(), f\\\"File not found: {CSV_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e852f2",
   "metadata": {},
   "source": [
    "## 2) Load CSV & quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b49740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "display(df.head())\n",
    "\n",
    "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "for c in label_cols:\n",
    "    assert c in df.columns, f\"Missing label column: {c}\"\n",
    "assert 'comment_text' in df.columns, \"Missing 'comment_text' column\"\n",
    "\n",
    "# Ensure labels are 0/1 ints\n",
    "for c in label_cols:\n",
    "    df[c] = df[c].astype(int)\n",
    "\n",
    "# Basic stats\n",
    "print(\"\\nLabel positives:\")\n",
    "print(df[label_cols].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94386705",
   "metadata": {},
   "source": [
    "## 3) Light text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84692f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "URL_RE = re.compile(r'http\\S+|www\\.\\S+')\n",
    "USER_RE = re.compile(r'@\\w+')\n",
    "HTML_RE = re.compile(r'<.*?>')\n",
    "SPACE_RE= re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.lower()\n",
    "    s = URL_RE.sub(' URL ', s)\n",
    "    s = USER_RE.sub(' USER ', s)\n",
    "    s = HTML_RE.sub(' ', s)\n",
    "    s = SPACE_RE.sub(' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df['clean_text'] = df['comment_text'].astype(str).apply(clean_text)\n",
    "display(df[['comment_text','clean_text']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c49f7f",
   "metadata": {},
   "source": [
    "## 4) Tokenize & pad sequences (Keras Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580165d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='[OOV]')\n",
    "tokenizer.fit_on_texts(df['clean_text'].tolist())\n",
    "\n",
    "def texts_to_padded(texts):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(seqs, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_all = texts_to_padded(df['clean_text'].tolist())\n",
    "y_all = df[label_cols].values.astype('float32')\n",
    "\n",
    "print(\"Tokenized:\", X_all.shape, y_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9a1ec",
   "metadata": {},
   "source": [
    "## 5) Train/Val/Test split (80/10/10, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "any_toxic = (y_all.sum(axis=1) > 0).astype(int)\n",
    "\n",
    "# First split train vs temp\n",
    "X_train, X_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(\n",
    "    X_all, y_all, np.arange(len(y_all)), test_size=0.2, random_state=SEED, stratify=any_toxic\n",
    ")\n",
    "\n",
    "# Split temp into val/test\n",
    "any_toxic_temp = (y_temp.sum(axis=1) > 0).astype(int)\n",
    "X_val, X_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
    "    X_temp, y_temp, idx_temp, test_size=0.5, random_state=SEED, stratify=any_toxic_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f972fd9",
   "metadata": {},
   "source": [
    "## 6) (Optional) Load GloVe embeddings (skip if not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "GLOVE_PATH = Path(\"/mnt/data/glove.6B.100d.txt\")  # put file here to enable\n",
    "emb_matrix = None\n",
    "\n",
    "if GLOVE_PATH.exists():\n",
    "    print(\"Loading GloVe from\", GLOVE_PATH)\n",
    "    embeddings_index = {}\n",
    "    with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    word_index = tokenizer.word_index\n",
    "    num_tokens = min(MAX_WORDS, len(word_index) + 1)\n",
    "    emb_matrix = np.random.normal(0, 0.6, (num_tokens, EMBED_DIM)).astype('float32')\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_WORDS: \n",
    "            continue\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            emb_matrix[i] = vec\n",
    "    print(\"Built embedding matrix:\", emb_matrix.shape)\n",
    "else:\n",
    "    print(\"GloVe not found; will train embeddings from scratch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e86f5",
   "metadata": {},
   "source": [
    "## 7) Build Text CNN model (lazy-import TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1585b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Speed up TF import/startup for CPU-only\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"     # skip GPU probing if no CUDA\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"2\"  # optional: tune threads\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"]  = \"4\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Embedding, Conv1D, GlobalMaxPooling1D,\n",
    "                                     Concatenate, Dense, Dropout)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def build_text_cnn(max_words, max_len, embed_dim, emb_matrix=None, num_labels=6, filters=128, drop=0.5):\n",
    "    inp = Input(shape=(max_len,), name=\"tokens\")\n",
    "    if emb_matrix is not None:\n",
    "        emb = Embedding(input_dim=emb_matrix.shape[0],\n",
    "                        output_dim=emb_matrix.shape[1],\n",
    "                        weights=[emb_matrix],\n",
    "                        input_length=max_len,\n",
    "                        trainable=False, name=\"embedding\")(inp)\n",
    "    else:\n",
    "        emb = Embedding(input_dim=max_words,\n",
    "                        output_dim=embed_dim,\n",
    "                        input_length=max_len,\n",
    "                        name=\"embedding\")(inp)\n",
    "\n",
    "    convs = []\n",
    "    for k in [3,4,5]:\n",
    "        c = Conv1D(filters, kernel_size=k, activation='relu', padding='valid')(emb)\n",
    "        p = GlobalMaxPooling1D()(c)\n",
    "        convs.append(p)\n",
    "    x = Concatenate()(convs)\n",
    "    x = Dropout(drop)(x)\n",
    "    out = Dense(num_labels, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(2e-3),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.AUC(name='auc', multi_label=True)])\n",
    "    return model\n",
    "\n",
    "model = build_text_cnn(MAX_WORDS, MAX_LEN, EMBED_DIM, emb_matrix, num_labels=len(label_cols))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f8d56",
   "metadata": {},
   "source": [
    "## 8) Train (EarlyStopping on val AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', mode='max', patience=2, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_auc', mode='max', factor=0.5, patience=1, min_lr=1e-5, verbose=1)\n",
    "]\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048edec",
   "metadata": {},
   "source": [
    "## 9) Evaluate on validation & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d73eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_probs(y_true, y_prob, label_cols):\n",
    "    # ROC-AUC per class\n",
    "    aucs = {}\n",
    "    for i, lab in enumerate(label_cols):\n",
    "        try:\n",
    "            aucs[lab] = roc_auc_score(y_true[:, i], y_prob[:, i])\n",
    "        except ValueError:\n",
    "            aucs[lab] = np.nan\n",
    "    macro_auc = np.nanmean(list(aucs.values()))\n",
    "    # F1 at 0.5 threshold\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    report = classification_report(y_true, y_pred, target_names=label_cols, zero_division=0)\n",
    "    return aucs, macro_auc, report\n",
    "\n",
    "# Validation\n",
    "y_val_prob = model.predict(X_val, batch_size=256, verbose=0)\n",
    "val_aucs, val_macro_auc, val_report = evaluate_probs(y_val, y_val_prob, label_cols)\n",
    "print(\"Validation ROC-AUC per class:\", val_aucs)\n",
    "print(f\"Validation Macro ROC-AUC: {val_macro_auc:.4f}\")\n",
    "print(\"\\nValidation classification report (thr=0.5):\\n\", val_report)\n",
    "\n",
    "# Test\n",
    "y_test_prob = model.predict(X_test, batch_size=256, verbose=0)\n",
    "test_aucs, test_macro_auc, test_report = evaluate_probs(y_test, y_test_prob, label_cols)\n",
    "print(\"Test ROC-AUC per class:\", test_aucs)\n",
    "print(f\"Test Macro ROC-AUC: {test_macro_auc:.4f}\")\n",
    "print(\"\\nTest classification report (thr=0.5):\\n\", test_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd62d7d",
   "metadata": {},
   "source": [
    "## 10) Tune per-class thresholds (maximize F1 on validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimal_thresholds(probs, targets, steps=200):\n",
    "    thrs = []\n",
    "    for i in range(probs.shape[1]):\n",
    "        best_f1, best_t = 0.0, 0.5\n",
    "        for t in np.linspace(0.05, 0.95, steps):\n",
    "            f1 = f1_score(targets[:, i], (probs[:, i] >= t).astype(int), zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "        thrs.append(best_t)\n",
    "    return np.array(thrs)\n",
    "\n",
    "thr_vec = optimal_thresholds(y_val_prob, y_val)\n",
    "print(\"Per-class optimal thresholds:\")\n",
    "print(dict(zip(label_cols, np.round(thr_vec, 3))))\n",
    "\n",
    "# Re-evaluate on test with tuned thresholds\n",
    "y_test_pred_tuned = (y_test_prob >= thr_vec).astype(int)\n",
    "print(\"\\nTest report with tuned thresholds:\")\n",
    "print(classification_report(y_test, y_test_pred_tuned, target_names=label_cols, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a625ba3",
   "metadata": {},
   "source": [
    "## 11) Save artifacts (model, tokenizer, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use Keras v3 format to avoid HDF5 dependency\n",
    "model_path = ART_DIR / \"text_cnn_toxic.keras\"\n",
    "tok_path   = ART_DIR / \"tokenizer.json\"\n",
    "thr_path   = ART_DIR / \"thresholds.npy\"\n",
    "\n",
    "with open(tok_path, \"w\") as f:\n",
    "    f.write(tokenizer.to_json())\n",
    "np.save(thr_path, thr_vec)\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "print(\"Saved model to:\", model_path.resolve())\n",
    "print(\"Saved tokenizer to:\", tok_path.resolve())\n",
    "print(\"Saved thresholds to:\", thr_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918c67e",
   "metadata": {},
   "source": [
    "## 12) Inference helper (single text or list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_texts(texts, model, tokenizer, max_len, thresholds=None):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    # reuse cleaner\n",
    "    cleaned = [clean_text(t) for t in texts]\n",
    "    seqs = tokenizer.texts_to_sequences(cleaned)\n",
    "    X = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "    probs = model.predict(X, verbose=0)\n",
    "    if thresholds is None:\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "    else:\n",
    "        preds = (probs >= thresholds).astype(int)\n",
    "    return probs, preds\n",
    "\n",
    "sample_texts = [\n",
    "    \"Thanks for the detailed explanation, much appreciated!\",\n",
    "    \"You're clueless and incompetent, just stop talking.\",\n",
    "    \"Keep talking and you'll regret it.\"\n",
    "]\n",
    "probs, preds = predict_texts(sample_texts, model, tokenizer, MAX_LEN, thresholds=thr_vec)\n",
    "print(\"Sample predictions (probabilities):\\n\", probs.round(3))\n",
    "print(\"Sample predictions (labels):\\n\", preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888fe45",
   "metadata": {},
   "source": [
    "## 13) (Optional) Baseline: TF‑IDF + Logistic Regression (fast sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89825e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Small baseline on train/val split only (to keep it quick)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=100000, min_df=2)\n",
    "X_tr_tfidf = tfidf.fit_transform(df.loc[idx_train, 'clean_text'].tolist())\n",
    "X_va_tfidf = tfidf.transform(df.loc[idx_val, 'clean_text'].tolist())\n",
    "\n",
    "lr_reports = {}\n",
    "for i, lab in enumerate(label_cols):\n",
    "    lr = LogisticRegression(max_iter=200, n_jobs=None)\n",
    "    lr.fit(X_tr_tfidf, y_train[:, i])\n",
    "    val_prob = lr.predict_proba(X_va_tfidf)[:, 1]\n",
    "    auc = roc_auc_score(y_val[:, i], val_prob)\n",
    "    pred = (val_prob >= 0.5).astype(int)\n",
    "    f1 = f1_score(y_val[:, i], pred, zero_division=0)\n",
    "    lr_reports[lab] = {\"auc\": round(auc, 4), \"f1@0.5\": round(f1, 4)}\n",
    "lr_reports\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
